# -*- coding: utf-8 -*-
"""Submission - LTSM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19j_VCj74COEgYHLH4etJkBvD1sjxmLDi
"""

! chmod 600 /content/kaggle.json

! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d venky73/spam-mails-dataset

import zipfile
zip_file = zipfile.ZipFile('/content/spam-mails-dataset.zip')
zip_file.extractall('/tmp/')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import re

import pickle 
#import mglearn
import time


from tensorflow.keras.preprocessing.text import Tokenizer
import nltk
from nltk import Text
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import word_tokenize  
from nltk.tokenize import sent_tokenize 
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression 
from sklearn.naive_bayes import MultinomialNB
from sklearn.multiclass import OneVsRestClassifier


from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline

from sklearn.metrics import accuracy_score
from sklearn.svm import LinearSVC

import string
import nltk
nltk.download('stopwords')
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""### IMPORT DATA"""

df = pd.read_csv('/tmp/spam_ham_dataset.csv')
df.drop(['label_num','Unnamed: 0'],axis=1, inplace=True)
df.head()

"""### Data Cleansing"""

# Cek apakah ada nilai Null
df.isnull().sum()

# Membuat Aggregat
df['Count']=1
df[['label','Count']].groupby(['label'], as_index=False).count().shape[0]

# Cek Jumlah label / Persebaran label
df[['label','Count']].groupby(['label'],as_index=False).count().sort_values(['Count'], ascending=False).head(10)

import cleantext
# Fungsi Cleansing text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub(r"\':", "", text)
    text = re.sub(r"\'subject :", " ", text)
    #text = re.sub('\W', ' ', text)
    #text = re.sub('\s+', ' ', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = cleantext.clean(text)
    # text = cleantext.clean_words(text)
    text = text.strip(' ')
    return text

list(df['text'][10:12].apply(clean_text))

df['textClean'] = df['text'].apply(clean_text)
df['textClean']= df['textClean'].str.replace('subject',"")
df.head()

from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import PorterStemmer

stop_words=stopwords.words('english')
stemmer=PorterStemmer()

X = df['textClean']

import re
cleaned_data=[]
for i in range(len(X)):
   text=re.sub('[^a-zA-Z]',' ',X.iloc[i])
   text=text.lower().split()
   text=[stemmer.stem(word) for word in text if (word not in stop_words)]
   text=' '.join(text)
   cleaned_data.append(text)

df['textClean'] = cleaned_data

# Cek Dataframe
df[['text','textClean','label']][6:12]

# Membuat Dataframe Baru / Data Clean
df_baru = pd.concat([df, df.label.apply(lambda x: ''.join(x)).str.get_dummies()], axis=1)
df_baru.drop(['text','label','Count'],axis=1, inplace=True)

df_baru

df_baru.columns

judul = df_baru['textClean'].values
label = df_baru[['ham', 'spam']].values

label

df_baru.head()

"""### Split Data"""

from sklearn.model_selection import train_test_split
judul_latih, judul_test, label_latih, label_test = train_test_split(judul, label, test_size=0.2,  random_state=42, shuffle=True)

# Tokenization
vocab_size = 10000
max_len = 100
trunc_type = "post"
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(judul_latih)

word_index = tokenizer.word_index

sequences_train = tokenizer.texts_to_sequences(judul_latih)
sequences_test = tokenizer.texts_to_sequences(judul_test)
pad_train = pad_sequences(sequences_train, maxlen=max_len, truncating=trunc_type)
pad_test = pad_sequences(sequences_test, maxlen=max_len, truncating=trunc_type)

print(pad_test.shape)

pad_train

# Model Embedding
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.optimizers import Adam
import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),
    tf.keras.layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5, return_sequences=True),
    tf.keras.layers.LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=False),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.compile(optimizer=Adam(lr=0.005), metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

#callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.75 and logs.get('val_accuracy')>0.75):
      self.model.stop_training = True
      print("\n akurasi dari training set and the validation set telah terpenuhi > 75%!")
callbacks = myCallback()

num_epochs = 25
history = model.fit(pad_train, label_latih, epochs=num_epochs, 
                    validation_data=(pad_test, label_test), verbose=2, callbacks=[callbacks])

